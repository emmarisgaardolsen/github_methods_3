---
title: "practical_exercise_8 , Methods 3, 2021, autumn semester"
author: 'Emma Olsen'
date: "17th of November"
output: pdf_document
---

<style type="text/css">
  body{
  font-size: 14pt;
}
</style>

```{r}

pacman::p_load(reticulate, Rcpp)
setwd("~/OneDrive - Aarhus Universitet/Methods3/github_methods_3/week_08")

```


# Exercises and objectives

1) Load the magnetoencephalographic recordings and do some initial plots to understand the data  
2) Do logistic regression to classify pairs of PAS-ratings  
3) Do a Support Vector Machine Classification on all four PAS-ratings  

REMEMBER: In your report, make sure to include code that can reproduce the answers requested in the exercises below (__MAKE A KNITTED VERSION__)  
REMEMBER: This is Assignment 3 and will be part of your final portfolio   

# EXERCISE 1 - Load the magnetoencephalographic recordings and do some initial plots to understand the data  

The files `megmag_data.npy` and `pas_vector.npy` can be downloaded here (http://laumollerandersen.org/data_methods_3/megmag_data.npy) and here (http://laumollerandersen.org/data_methods_3/pas_vector.npy)   

1) Load `megmag_data.npy` and call it `data` using `np.load`. You can use `join`, which can be imported from `os.path`, to create paths from different string segments. 

```{python}

import numpy as np 
import pandas as pd

data = np.load('megmag_data.npy')
data.shape

print(data)

```


  i. The data is a 3-dimensional array. The first dimension is number of repetitions of a visual stimulus , the second dimension is the number of sensors that record magnetic fields (in Tesla) that stem from neurons activating in the brain, and the third dimension is the number of time samples. How many repetitions, sensors and time samples are there?  

```{python}

data.shape 
  # first dimension (682), number of repetitions of a visual stimuli
  # 2nd dimension (102), n of sensors recording magnetic fields 
  # 3rd dimension (251), n of time samples

```

  ii. The time range is from (and including) -200 ms to (and including) 800 ms with a sample recorded every 4 ms. At time 0, the visual stimulus was briefly presented. Create a 1-dimensional array called `times` that represents this.  

*Numbers from -200 to 800 in intervals of 4*
```{python}

time = np.arange(-200, 804,4) # start fra 200, slut ved 801 (fordi python starter fra nul), og spring et skridt på 4 hver gang

```

*time har index for tid, men siger ikke hvad tiden er*
*time range går fra -200 til 800 ms*
*vi skal have en range fra -200 til 800 ms (med sample hvert 4 ms). using* 
*vi skal lave en vector, der har begynder på -200, slutter på 800, og har en værdi hver 4. milisecond - der findes -200, -196, -192*
*vi vil se når time er 0*
  
  iii. Create the sensor covariance matrix $\Sigma_{XX}$: $$\Sigma_{XX} = \frac 1 N \sum_{i=1}^N XX^T$$ $N$ is the number of repetitions and $X$ has $s$ rows and $t$ columns (sensors and time), thus the shape is $X_{s\times t}$. 
  
Do the sensors pick up independent signals? (Use `plt.imshow` to plot the sensor covariance matrix)  

```{python}

# THIS IS JUST A TEST CHUNK

# X is a matrix, s*t 
  # number of rows (s) sensors = 102
  # number of columns (t) time = 251

data.shape 
skrrt = data[3,:,:]
skrrt.shape
print(skrrt)

```

```{python}

n = 682
cov_mat = []

# Calculating the dot product for all rows i using all data points in the dimensions

for i in range(n):
    cov_mat.append(data[i,:,:] @ data[i,:,:].T)

# Out of the loop the dot product of the the matrices for each i is summed and divided by n.
cov_mat = sum(cov_mat)/n


# Plotting the covariance matrix
import matplotlib.pyplot as plt
plt.imshow(cov_mat)
plt.show(cov_mat)

```

*alt der ik er på diagonalen er correlated?*
*Do the sensors pick up independent signals? - No, there is correlation between the sensors*

  iv. Make an average over the repetition dimension using `np.mean` - use the `axis` argument. (The resulting array should have two dimensions with time as the first and magnetic field as the second)  
```{python}

import matplotlib.pyplot as plt

avr_rep = np.mean(data, axis=0) # repetition of visual stim
avr_rep.shape # 102 elements (avg activity per reps across sensors) in first dimension, 251 elements in dimension 2.  

print(avr_rep)

```

  v. Plot the magnetic field (based on the average) as it evolves over time for each of the sensors (a line for each) (time on the x-axis and magnetic field on the y-axis). Add a horizontal line at $y = 0$ and a vertical line at $x = 0$ using `plt.axvline` and `plt.axhline`  
  
```{python}

import matplotlib.pyplot as plt

plt.figure()
plt.plot(avr_rep.T) # avg brain activity across repetition
plt.axvline(x = 0)
plt.axhline(y = 0)
plt.ylabel('mag field')
plt.xlabel('time')
plt.title('name')
plt.show()

```

```{python}

# Putting our times on the x-axis? 

import matplotlib.pyplot as plt

#### WHY TRANSPOSED
plt.figure()
plt.plot(time,avr_rep.T) # if we didn't do this, we would plot time per magnetic field??? wtf. transposing shows magnetic field per time.. 
plt.figure()# Why transposed?
plt.axvline()
plt.axhline()
plt.xlabel('time')
plt.ylabel('mag field')
plt.title('magnetic field (based on the average)')
plt.show()

```

  vi. Find the maximal magnetic field in the average. Then use `np.argmax` and `np.unravel_index` to find the sensor that has the maximal magnetic field.  
```{python}

# The numpy.argmax() function returns indices of the max element of the array in a particular axis. 
# The np.unravel_inex converts a flat index or array of flat indices into a tuple of coordinate arrays

maxer = np.unravel_index(np.argmax(avr_rep), avr_rep.shape)
print(maxer) # sensor 73 has the maximal magnetic field (at repetition 112)

print(avr_rep[73,112]) # the max value of the magnetic field is 2.7886216843591933e-13
# testing that it corresponds
np.amax(avr_rep) # 2.7886216843591933e-13

print(time[112]) 

```
  
The sensor receiving the strongest is number 73. It happens at repetition 112 (248 miliseconds after the stimulus onset)


  vii. Plot the magnetic field for each of the repetitions (a line for each) for the sensor that has the maximal magnetic field. Highlight the time point with the maximal magnetic field in the average (as found in 1.1.v) using `plt.axvline`  

```{python}

# sensor 73 has the maximal magnetic field (at repetition 112) of 2.7886216843591933e-13..... but at which time point is this?

# Plot the magnetic field for each of the repetitions (a line for each) for the sensor that has the maximal magnetic field (sensor 73)
plt.figure()
plt.plot(time, data[:,73,:].T)
plt.axvline(x = 0) #Highlight the time point with the maximal magnetic field in the average
plt.axhline(y = 0)
plt.axvline(x = 248, color = "red")
plt.xlabel("Time (in ms)")
plt.ylabel("magnetic field")
plt.title("magnetic field (based on the average)")
plt.show()

```
  
  viii. Describe in your own words how the response found in the average is represented in the single repetitions. But do make sure to use the concepts _signal_ and _noise_ and comment on any differences on the range of values on the y-axis  

*blue: this is when stim is shown. mean: max avg?*

*?????*
*?????*
*?????*

```{python}


```
  
  
2) Now load `pas_vector.npy` (call it `y`). PAS is the same as in Assignment 2, describing the clarity of the subjective experience the subject reported after seeing the briefly presented stimulus  

```{python}
# loading the data
y = np.load("pas_vector.npy")
len(y)
```

  i. Which dimension in the `data` array does it have the same length as? 

*It has the same length as the first dimension i.e., number of repetitions of a visual stimuli*
  
  ii. Now make four averages (As in Exercise 1.1.iii), one for each PAS rating, and plot the four time courses (one for each PAS rating) for the sensor found in Exercise 1.1.v  (1.1.v.i)
  
```{python}

# The sensor found in 1.1.v was sensor 73
sensor_73 = data[:,73,:]

############## MAKING 4 AVERAGES ############## 

#
PAS_1= np.where(y == 1) # index where PAS rating is 1 
PAS_2= np.where(y == 2) 
PAS_3= np.where(y == 3) 
PAS_4= np.where(y == 4) 


avgrepPAS_1 = np.mean(sensor_73[PAS_1], axis= 0)
avgrepPAS_2 = np.mean(sensor_73[PAS_2], axis= 0)
avgrepPAS_3 = np.mean(sensor_73[PAS_3], axis= 0)
avgrepPAS_4 = np.mean(sensor_73[PAS_4], axis= 0)



############## plotting the four time courses (one for each PAS rating) for the sensor found in Exercise 1.1.v ############## 
plt.figure()
plt.plot(time,avgrepPAS_1)
plt.plot(time,avgrepPAS_2)
plt.plot(time,avgrepPAS_3)
plt.plot(time,avgrepPAS_4)
plt.axvline()
plt.axhline()
plt.xlabel("time")
plt.ylabel("magnetic field")
plt.title("magnetic field for each PAS-rating (based on the average)")
plt.legend(["pas 1", "pas 2", "pas 3", "pas 4"])
plt.show()

```


  iii. Notice that there are two early peaks (measuring visual activity from the brain), one before 200 ms and one around 250 ms. Describe how the amplitudes of responses are related to the four PAS-scores. Does PAS 2 behave differently than expected?  
*Avg brain activity for sensor 73. Across repetitions but divided according to PAS rating*
*The amplitude of PAS2 is quite high, considering that it corresponds to only a weak glimpse (i.e. "A feeling that something has been shown. Not characterized by any content, and this cannot be specified any further).*


# EXERCISE 2 - Do logistic regression to classify pairs of PAS-ratings  

1) Now, we are going to do Logistic Regression with the aim of classifying the PAS-rating given by the subject  
  i. We'll start with a binary problem - create a new array called `data_1_2` that only contains PAS responses 1 and 2. Similarly, create a `y_1_2` for the target vector  
  
```{python}

# Create a new array called `data_1_2` 
data_1_2 = np.concatenate((data[PAS_1], data[PAS_2]), axis=0) # take only PAS responses 1 and 2 - choosing axis 0 which is our first dimension, i.e. repetition
data_1_2.shape # (214, 102, 251) - correct motherfucker!! 
data_1_2.ndim

y_1_2 = np.concatenate((y[PAS_1],y[PAS_2]),axis=0)

# create a `y_1_2` for the target vector
#y_1_2 = [] # empty element

#for i in range(len(y)):  # for each element in y 
 ##      y_1_2.append(i)
   # if y[i] == 2: # if y == 1, append the variable to the empty list
    #    y_1_2.append(i)

#len(y_1_2) # this gives 214. Makes sense as PAS1 is 99, and PAS2 is 115 

```
  
  ii. Scikit-learn expects our observations (`data_1_2`) to be in a 2d-array, which has samples (repetitions) on dimension 1 and features (predictor variables) on dimension 2. Our `data_1_2` is a three-dimensional array. Our strategy will be to collapse our two last dimensions (sensors and time) into one dimension, while keeping the first dimension as it is (repetitions). Use `np.reshape` to create a variable `X_1_2` that fullfils these criteria.  
  
```{python}
# dim 1: repetition[dim 0]
# dim 2: sensor & time [dim 1-2]

######################## WTFFF??? ####
#  Use `np.reshape` to create a variable `X_1_2` that fullfils these criteria.  ?????? ---> didn't do this??

X_1_2 = data_1_2.reshape(data_1_2.shape[0],(data_1_2.shape[1]*data_1_2.shape[2])) ##(2 other dimensions collapsed, first dimension)
X_1_2.shape # 214, 25602 

```

*X_1_2.shape*


  iii. Import the `StandardScaler` and scale `X_1_2`  
  

```{python}

from sklearn.preprocessing import StandardScaler # apply centering (subtracting the mean and standardizing, bringing everything to the same scale)
sc = StandardScaler()
X_1_2_sc = sc.fit_transform(X_1_2) 
X_1_2_sc.shape

```

  iv. Do a standard `LogisticRegression` - can be imported from `sklearn.linear_model` - make sure there is no `penalty` applied  
  
```{python}

from sklearn.linear_model import LogisticRegression
logR = LogisticRegression(penalty='none') # no regularisation
logR.fit(X_1_2_sc, y_1_2)

intercept = logR.intercept_;print(intercept) # intercept
slope = logR.coef_;print(slope) # slope

```


  v. Use the `score` method of `LogisticRegression` to find out how many labels were classified correctly. Are we overfitting? Besides the score, what would make you suspect that we are overfitting?  
```{python}

print('Training accuracy:',logR.score(X_1_2_sc, y_1_2)) # 1, classifying all labels correctly. 

```

*A classification error of 0 suggest that we are overfitting, especially taking into consideration the fact that our model doesn't include any penalty.*


  vi. Now apply the _L1_ penalty instead - how many of the coefficients (`.coef_`) are non-zero after this?  
  
*L1 regularization is a penalty against model complexity. It means replacing the square of the weights by the sum of the absolute values of the weights and does often yield sparse feature vectors where most feature weights are zero. This is practical in a high dimensional dataset with many irrelevant features.. So it is useful for feature selection!*

```{python}

from sklearn.linear_model import LogisticRegression

# logPen = LogisticRegression(penalty='l1', solver='saga', tol=0.01, c=1) # not working
logPen = LogisticRegression(random_state = 1, penalty='l1', solver='liblinear') 
logPen.fit(X_1_2_sc, y_1_2)
print('Training accuracy:',logPen.score(X_1_2_sc, y_1_2))
print(logPen.coef_)



coefs = logPen.coef_.flatten() # make 2D array to a list
coefs
non_zero_indices = coefs != 0 
non_zero_indices

X_1_2_sc.shape
X_reduced = X_1_2_sc[:,non_zero_indices].shape



```

*4406 wtf???*  
  
  vii. Create a new reduced $X$ that only includes the non-zero coefficients - show the covariance of the non-zero features (two covariance matrices can be made; $X_{reduced}X_{reduced}^T$ or $X_{reduced}^TX_{reduced}$ (you choose the right one)). Plot the covariance of the features using `plt.imshow`. Compared to the plot from 1.1.iii, do we see less covariance?  

* vi har x is*
```{python}

covmat = X_reduced.T @ X_reduced
covmat.shape

import matplotlib.pyplot as plt
plt.imshow(covmat)
plt.show(covmat)

```


2) Now, we are going to build better (more predictive) models by using cross-validation as an outcome measure    
  i. Import `cross_val_score` and `StratifiedKFold` from `sklearn.model_selection`  
  ii. To make sure that our training data sets are not biased to one target (PAS) or the other, create `y_1_2_equal`, which should have an equal number of each target. Create a similar `X_1_2_equal`. The function `equalize_targets_binary` in the code chunk associated with Exercise 2.2.ii can be used. Remember to scale `X_1_2_equal`!  
  iii. Do cross-validation with 5 stratified folds doing standard `LogisticRegression` (See Exercise 2.1.iv)  
  iv. Do L2-regularisation with the following `Cs=  [1e5, 1e1, 1e-5]`. Use the same kind of cross-validation as in Exercise 2.2.iii. In the best-scoring of these models, how many more/fewer predictions are correct (on average)?  
  v. Instead of fitting a model on all `n_sensors * n_samples` features, fit  a logistic regression (same kind as in Exercise 2.2.iv (use the `C` that resulted in the best prediction)) for __each__ time sample and use the same cross-validation as in Exercise 2.2.iii. What are the time points where classification is best? Make a plot with time on the x-axis and classification score on the y-axis with a horizontal line at the chance level (what is the chance level for this analysis?)  
  vi. Now do the same, but with L1 regression - set `C=1e-1` - what are the time points when classification is best? (make a plot)?  
  vii. Finally, fit the same models as in Exercise 2.2.vi but now for `data_1_4` and `y_1_4` (create a data set and a target vector that only contains PAS responses 1 and 4). What are the time points when classification is best? Make a plot with time on the x-axis and classification score on the y-axis with a horizontal line at the chance level (what is the chance level for this analysis?)  


3) Is pairwise classification of subjective experience possible? Any surprises in the classification accuracies, i.e. how does the classification score fore PAS 1 vs 4 compare to the classification score for PAS 1 vs 2?  


```{python, eval=FALSE}
# Exercise 2.2.ii
def equalize_targets_binary(data, y):
    np.random.seed(7)
    targets = np.unique(y) ## find the number of targets
    if len(targets) > 2:
        raise NameError("can't have more than two targets")
    counts = list()
    indices = list()
    for target in targets:
        counts.append(np.sum(y == target)) ## find the number of each target
        indices.append(np.where(y == target)[0]) ## find their indices
    min_count = np.min(counts)
    # randomly choose trials
    first_choice = np.random.choice(indices[0], size=min_count, replace=False)
    second_choice = np.random.choice(indices[1], size=min_count,replace=False)
    
    # create the new data sets
    new_indices = np.concatenate((first_choice, second_choice))
    new_y = y[new_indices]
    new_data = data[new_indices, :, :]
    
    return new_data, new_y
```

# EXERCISE 3 - Do a Support Vector Machine Classification on all four PAS-ratings  
1) Do a Support Vector Machine Classification  
    i. First equalize the number of targets using the function associated with each PAS-rating using the function associated with Exercise 3.1.i  
    ii. Run two classifiers, one with a linear kernel and one with a radial basis (other options should be left at their defaults) - the number of features is the number of sensors multiplied the number of samples. Which one is better predicting the category?
    iii. Run the sample-by-sample analysis (similar to Exercise 2.2.v) with the best kernel (from Exercise 3.1.ii). Make a plot with time on the x-axis and classification score on the y-axis with a horizontal line at the chance level (what is the chance level for this analysis?)
    iv. Is classification of subjective experience possible at around 200-250 ms?  
2) Finally, split the equalized data set (with all four ratings) into a training part and test part, where the test part if 30 % of the trials. Use `train_test_split` from `sklearn.model_selection`  
    i. Use the kernel that resulted in the best classification in Exercise 3.1.ii and `fit`the training set and `predict` on the test set. This time your features are the number of sensors multiplied by the number of samples.  
    ii. Create a _confusion matrix_. It is a 4x4 matrix. The row names and the column names are the PAS-scores. There will thus be 16 entries. The PAS1xPAS1 entry will be the number of actual PAS1, $y_{pas1}$ that were predicted as PAS1, $\hat y_{pas1}$. The PAS1xPAS2 entry will be the number of actual PAS1, $y_{pas1}$ that were predicted as PAS2, $\hat y_{pas2}$ and so on for the remaining 14 entries.  Plot the matrix
    iii. Based on the confusion matrix, describe how ratings are misclassified and if that makes sense given that ratings should measure the strength/quality of the subjective experience. Is the classifier biased towards specific ratings?  
    
```{python, eval=FALSE}

def equalize_targets(data, y):
    np.random.seed(7)
    targets = np.unique(y)
    counts = list()
    indices = list()
    for target in targets:
        counts.append(np.sum(y == target))
        indices.append(np.where(y == target)[0])
    min_count = np.min(counts)
    first_choice = np.random.choice(indices[0], size=min_count, replace=False)
    second_choice = np.random.choice(indices[1], size=min_count, replace=False)
    third_choice = np.random.choice(indices[2], size=min_count, replace=False)
    fourth_choice = np.random.choice(indices[3], size=min_count, replace=False)
    
    new_indices = np.concatenate((first_choice, second_choice,
                                 third_choice, fourth_choice))
    new_y = y[new_indices]
    new_data = data[new_indices, :, :]
    
    return new_data, new_y

```