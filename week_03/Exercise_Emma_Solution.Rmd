---
title: "Exercise"
output: html_document
---

---

title: "practical_exercise_3, Methods 3, 2021, autumn semester"
author: '[FILL IN YOUR NAME]'
date: "[FILL IN THE DATE]"
output: html_document
---

<style type="text/css">
  body{
  font-size: 14pt;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(tidyverse, readbulk, patchwork, lmerTest, lme4)
```

# Exercises and objectives
The objectives of the exercises of this assignment are:  
1) Download and organise the data and model and plot staircase responses based on fits of logistic functions  
2) Fit multilevel models for response times  
3) Fit multilevel models for count data  

REMEMBER: In your report, make sure to include code that can reproduce the answers requested in the exercises below (__MAKE A KNITTED VERSION__)  
REMEMBER: This assignment will be part of your final portfolio

## Exercise 1

Go to https://osf.io/ecxsj/files/ and download the files associated with Experiment 2 (there should be 29).  
The data is associated with Experiment 2 of the article at the following DOI https://doi.org/10.1016/j.concog.2019.03.007  

1) Put the data from all subjects into a single data frame 

```{r, include=FALSE}

setwd("/Users/emmaolsen/OneDrive - Aarhus Universitet/Methods3/github_methods_3/week_03/experiment_2")

df <- read_bulk(
  directory = '/Users/emmaolsen/OneDrive - Aarhus Universitet/Methods3/github_methods_3/week_03/experiment_2',
  fun = read_csv
  )

head(df)

```


2) Describe the data and construct extra variables from the existing variables 
    i. add a variable to the data frame and call it _correct_ (have it be a _logical_ variable). Assign a 1 to each row where the subject indicated the correct answer and a 0 to each row where the subject indicated the incorrect answer (__Hint:__ the variable _obj.resp_ indicates whether the subject answered "even", _e_ or "odd", _o_, and the variable _target_type_ indicates what was actually presented.
  
*obj.resp [i,16], o, e*
*target_type [i,11], odd, even*
*correct [i,19]*
```{r}

df$correct <- ifelse((df$obj.resp == "o" & df$target.type=="odd") | (df$obj.resp == "e" & df$target.type=="even"), 1,0)

```

    
  ii. describe what the following variables in the data frame contain, _trial.type_, _pas_, _trial_, _target.contrast_, _cue_, _task_, _target_type_, _rt.subj_, _rt.obj_, _obj.resp_, _subject_ and _correct_. (That means you can ignore the rest of the variables in your description). For each of them, indicate and argue for what `class` they should be classified into, e.g. _factor_, _numeric_ etc.  


*_trial.type_*: a character variable encoded as either "staircase" (i.e. data was collected as a part of the adaptive staircase procedure which was performed at the beginning of the study) or "experiment" (i.e. data was collected as a part of the actual experiment trials). It should be re-encoded a factor in order for _trial.type_ to be modelled later in the analysis, if relevant. However, if we were only interested in the experimental trials, we might consider filtering out all staircase trials*.
```{r}
df$trial.type <- as.factor(df$trial.type)
```

*_pas_*: indicates one of the 4 categorically different ratings of the Perceptual Awareness Scale (PAS: Ramsøy & Overgaard, 2004), rated by the participant. It indicates the participant's perceptual awareness of the stimulus where the numbers represent the following:* 

*(1) No experience* (i.e. "no impression of the simulus. All answes are seen as mere guesses)*

*(2) Weak Glimpse*  (i.e. "A feeling that something has been shown. Not characterized by any content, and this cannot be specified any further)*

*(3) Almost Clear Experience* (i.e. "Ambiguous experience of the stimulus. Some stimulus aspects are experienced more vividly than others. A feeling of almost being certain about one’s answer")*

*4) Clear experience* (i.e. "Non-ambiguous experience of the stimulus. No doubt in one’s answer")*

It should be encoded as a factor as it is a case of ordinal data.
```{r}
df$pas <- as.factor(df$pas)
```

*_trial_*: refers to the trial number. It is numeric but should be factor
```{r}
df$trial <- as.factor(df$trial)
```

*_target.contrast_*: indicates the contrast of the target stimulus, relative to the background, adjusted to match the threshold of each individual by using the QUEST-algorithm. It is numeric and should stay numeric as we are dealing with a continuous variable with values falling within the range of 0-1*

*_cue_*: cue shown to participants to indicate the set of possible digits that might appear on each trail. There were 36 different combination of cues (0-35). The task-column specifies the number of potential targets cued. As the _cue_ variable contains nominal data, it should be reencoded into a factor.

```{r}
df$cue <- as.factor(df$cue)
```


*_task_*: indicates the number of potential targets that were cued. Consisted of either 2,4, or 8 digits, indicated by the character variable *_task_* taking one of the 3 values:

  *singles* (2 possible targets) e.g. 2:9
  *pairs* (4 possible targets) e.g. 24:57
  *quadruplet* (8 possible targets) e.g. 2468:3579

This variable should also be re-encoded as a factor as we are, again, dealing with nominal data.
```{r}

df$task <- as.factor(df$task)

```

*target_type*: indicates the parity of the target stimulus (i.e. whether the digit was even or odd). It is encoded as a character but should be re-encoded as a factor. 
```{r}
df$target.type <- as.factor(df$target.type)
```

*rt.subj*: response time of answering Perceptual Awareness Scale
Encoded as numeric as it is continuous data.

*rt.obj*: response time (time taken by participant to indicate the parity of the target stimulus). 
Encoded as numeric as it is continuous data.

*obj.resp*: the participant's answer when asked to indicate the parity of the target stimulus (e = even, o = odd). Encoded as numeric but should be re-encoded as a factor.
```{r}
df$obj.resp <- as.factor(df$obj.resp)
```

*subject*: participant ID, enabling us to distinguish between data collected from different participants. Is a character variable but should be re-encoded as a factor as we are dealing with a repeated measures design and therefore need to be able to model subject as a random effect in the analysis phase.

```{r}
df$subject <- as.factor(df$subject)
```

*correct*: indicates accuracy of the respondent's answer of the target stimulus' parity. It is a binary variable where 0 indicates an incorrect response and 1 indicates a correct one, and should therefore be re-encoded as a factor. 

```{r}
df$correct <- as.factor(df$correct)
```

   iii. for the staircasing part __only__, create a plot for each subject where you plot the estimated function (on the _target.contrast_ range from 0-1) based on the fitted values of a model (use `glm`) that models _correct_ as dependent on _target.contrast_. These plots will be our _no-pooling_ model. Comment on the fits - do we have enough data to plot the logistic functions?  

```{r}
library(tidyverse)

# make subset of df with only trial.type == staircase
df_staircase <- subset(df, df$trial.type == "staircase")

```

```{r, function, running it in a loop}

df_staircase$subject = gsub("(?<![0-9])0+", "", df_staircase$subject, perl = TRUE)
df_staircase$subject = as.integer(df_staircase$subject)

nopoolfun <- function(i){
  dat <- df_staircase[which(df_staircase$subject == i),]
  model <- glm(correct~target.contrast, family = 'binomial', data=dat)
  fitted <- model$fitted.values
  plot_dat <- data.frame(cbind(fitted,'target.contrast'=dat$target.contrast))
plot <- ggplot(plot_dat, aes(x = target.contrast, y = fitted))+
  geom_point()+
  geom_line(aes(x = target.contrast, y = fitted))+
  xlab('Target Contrast')+
  ylim(c(0,1))+
  theme_minimal()
print(plot)
}

# Running the function for each participant
for (i in 1:29){
  nopoolfun(i)
}


library(gridExtra)

subjects <- c(1:16)
plots <- lapply(subjects, FUN=nopoolfun)
do.call(grid.arrange,  plots)

subjects <- c(17:29)
plots <- lapply(subjects, FUN=nopoolfun)
do.call(grid.arrange,  plots)



```

```{r}
#  --------------- No pooling model  --------------- 

m_nopool <- glm(correct~target.contrast*subject, family=binomial(link=logit), data=df_staircase)
fitted_nopool <- fitted(m_nopool)
df_staircase$fitted_nopool <- fitted_nopool

# one plot 
ggplot(data = df_staircase)+
  geom_point(aes(x = target.contrast, y = fitted(m_nopool), color = subject))+
  geom_line(aes(target.contrast, fitted(m_nopool))) +
  labs(title = "No pooling plot")+
  facet_wrap(.~subject)

```

From visual inspection of the *no pooling plot*, it becomes clear that the fits are very bad. A subset data frame containing only the staircasing data doesn't contain enough data to plot the logistic functions (the fitted values do not follow a sigmoid function).

  iv. on top of those plots, add the estimated functions (on the _target.contrast_ range from 0-1) for each subject based on partial pooling model (use `glmer` from the package `lme4`) where unique intercepts and slopes for _target.contrast_ are modelled for each _subject_  
  
```{r}

# Telling the model to expect differing baseline-levels of "correct" (the intercept, represented by 1) as well as differing responses to the main factor in question, which is "target.constrast" in this case.

m_partial <- glmer(correct~target.contrast + (1+target.contrast|subject), data = df_staircase, family = "binomial")
fitted_partial <- fitted(m_partial)
df_staircase$fitted_partial <- fitted_partial


## Plotting on top
ggplot(data = df_staircase)+
  geom_point(aes(x = target.contrast, y = fitted(m_nopool), color = "no pooling"))+
  geom_point(aes(x = target.contrast, y = fitted(m_partial), color= "partial pooling"))+
  geom_line(aes(target.contrast, fitted(m_partial), color = "partial pooling")) +
  geom_line(aes(target.contrast, fitted(m_nopool), color = "no pooling")) +
  labs(title = "No pooling and partial pooling model plot")+
  facet_wrap(.~subject)

```

  v. in your own words, describe how the partial pooling model allows for a better fit for each subject  

Partial pooling allows for a better fit for each subject as it takes into account the expected individual baseline differences depending on the stimulus contrast level as well as allows for different slopes for each subject. In other words, it allows the model to reflect that individuals would perform differently in different experimental settings. Partial pooling provides us a compromise between complete and no-pooling models, as it allows us to model both an average (general tendency of the whole dataset) and each level of the categorical predictor, subject. In partial pooling (a mixed effects model), we model both an average and each level, i.e. combine information from the population (fixed) effects of the complete pooling model and the subject-specific (random) effects of the no-pooling one. (((accounts for nested structure of our data, it assumes hierarchy in our data structure)))


## Exercise 2

Now we __only__ look at the _experiment_ trials (_trial.type_)  

1) Pick four subjects and plot their Quantile-Quantile (Q-Q) plots for the residuals of their objective response times (_rt.obj_) based on a model where only intercept is modelled  
 
```{r}
df_experiment <- df %>% filter(trial.type=="experiment")
df_experiment


df_experiment$subject = gsub("(?<![0-9])0+", "", df_experiment$subject, perl = TRUE)
df_experiment$subject = as.integer(df_experiment$subject)

# pick 4 random from data frame
library(dplyr)
set.seed(1)
sample <- sample_n(df_experiment,4) # note that you could get the same number twice
sample # 03, 019, 012, 025

# make df subsets for each subject
sub3 <- df_experiment %>% filter(subject==3)
sub19 <- df_experiment %>% filter(subject==19)
sub12 <- df_experiment %>% filter(subject==12)
sub25 <- df_experiment %>% filter(subject==25)

# make modes (lm cause continuous variables)
exp_model <- lm(rt.obj~1, data = df_experiment)   # overall

# FModels
m3 <- lm(rt.obj~1, data = sub3)
m19 <- lm(rt.obj~1, data = sub19)
m12 <- lm(rt.obj~1, data = sub12)
m25 <- lm(rt.obj~1, data = sub25)

# QQ-plot
qq3 <- ggplot(sub3, aes(sample = residuals(m3))) + stat_qq() +
stat_qq_line(colour = "red") +
labs(x = "Theoretical quantiles", y = "Sample quantiles") + ggtitle("Subject 003: Q-Q Plot for residuals") +
theme_bw()

qq19 <- ggplot(sub19, aes(sample = residuals(m19))) + stat_qq() +
stat_qq_line(colour = "red") + 
labs(x = "Theoretical quantiles", y = "Sample quantiles") + ggtitle("Subject 019: Q-Q Plot for residuals") +
theme_bw()

qq12 <- ggplot(sub12, aes(sample = residuals(m12))) + stat_qq() +
stat_qq_line(colour = "red") +
labs(x = "Theoretical quantiles", y = "Sample quantiles") + ggtitle("Subject 012: Q-Q Plot for residuals") +
theme_bw()

qq25 <- ggplot(sub25, aes(sample = residuals(m25))) + stat_qq() +
stat_qq_line(colour = "red") +
labs(x = "Theoretical quantiles", y = "Sample quantiles") + ggtitle("Subject 025: Q-Q Plot for residuals") +
theme_bw()


ggpubr::ggarrange(qq3, qq19, qq12, qq25)

```
 i. comment on these    
 All 4 qq-plots indicates major deviances from normality, i.e. right skewness. We could try to improve the QQ-plots via log-transformation of the response time data.
 
ii. does a log-transformation of the response time data improve the Q-Q-plots?  
```{r}

m3log <- lm(log(rt.obj)~1, data = sub3)
qq3log <- ggplot(sub3, aes(sample = residuals(m3log))) + stat_qq() +
stat_qq_line(colour = "red") +
labs(x = "Theoretical quantiles", y = "Sample quantiles") + ggtitle("Subj 3: QQ-plot residuals (log)") +
theme_bw()


m19log <- lm(log(rt.obj)~1, data = sub19)
qq19log <- ggplot(sub19, aes(sample = residuals(m19log))) + stat_qq() +
stat_qq_line(colour = "red") +
labs(x = "Theoretical quantiles", y = "Sample quantiles") + ggtitle("Subj 19: QQ-plot residuals") +
theme_bw()

m12log <- lm(log(rt.obj)~1, data = sub12)
qq12log <- ggplot(sub12, aes(sample = residuals(m12log))) + stat_qq() +
stat_qq_line(colour = "red") +
labs(x = "Theoretical quantiles", y = "Sample quantiles") + ggtitle("Subj 12:QQ-plot residuals") +
theme_bw()


m25log <- lm(log(rt.obj)~1, data = sub25)
qq25log <- ggplot(sub25, aes(sample = residuals(m25log))) + stat_qq() +
stat_qq_line(colour = "red") +
labs(x = "Theoretical quantiles", y = "Sample quantiles") + ggtitle("Subj 25: QQ-plot residuals") +
theme_bw()


ggpubr::ggarrange(qq3log, qq19log, qq12log, qq25log)

```

Doing a log-transformation improves the QQ-plots slightly, altough the data still deviates from normality. 

    
2) Now do a partial pooling model modelling objective response times as dependent on _task_? (set `REML=FALSE` in your `lmer`-specification)  

```{r}

partial1 <- lmerTest::lmer(log(rt.obj)~task + (1|subject) + (1|trial), data = df_experiment, REML=FALSE)
partial2 <- lmerTest::lmer(log(rt.obj)~task + (1|trial), data = df_experiment, REML=FALSE)
partial3 <- lmerTest::lmer(log(rt.obj)~task + (1|subject), data = df_experiment, REML=FALSE)

partial4 <- lmerTest::lmer(log(rt.obj) ~ task + (1 + task | subject) + (1 | trial), data = df_experiment, REML = F)
partial5 <- lmerTest::lmer(log(rt.obj) ~ task + (1 + task | subject), data = df_experiment, REML=FALSE)

partial_model <- c("m1", "m2", "m3", "m4", "m5")
AIC <- c(AIC(partial1), AIC(partial2), AIC(partial3), AIC(partial4), AIC(partial5))
sigma - c(sigma(partial1),sigma(partial2),sigma(partial3), sigma(partial4), sigma(partial5))

# calculating pseudo R-squared for mixed effects model (R2m = marginal R2, provides the variance explained only by fixed effects. R2c = conditional R2, provides the variance explained by the entire model, i.e., both fixed effects and random effects)

pacman::p_load(MuMIn)
r.squaredGLMM(partial1) # R2c = .2629
r.squaredGLMM(partial2) # R2c = .0360
r.squaredGLMM(partial3) # R2c = .2253
r.squaredGLMM(partial4) # R2c = .2750
r.squaredGLMM(partial5) # R2c = .2371

pseudo_r2 <- c(0.2629, 0.0360, 0.2253, 0.2750, 0.2371)
  

as_tibble(cbind(partial_model, sigma, AIC, pseudo_r2))

```

  i. which would you include among your random effects and why? (support your choices with relevant measures, taking into account variance explained and number of parameters going into the modelling)  

m4 has the lowest sigma as well as AIC value. This is the model that predicts objective response times dependent on task (fixed effects), modelling a random intercepts for subjects and trial as well as random slopes for task. Random intercepts were modeled for subject as one wouold expect individual baseline performance differences among different participants. Trial as also been modelled as random intercepts as you would expect different trials to vary in difficulty (and participants might modify their performance as they get acquainted with the experiment). 

  ii. explain in your own words what your chosen models says about response times between the different tasks  
  
```{r}
summary(partial4)
```

The estimates for quadruplet and singles are negative, which signifies that these tasks have a shorter reaction time compared to the pair task. Relative to the quadruplet the singles task is generally faster. 

The coefficients "taskquadruplet" and "tasksingles" are both negative and accompanied by small significant p-values, indicating that as the task changes from *pairs* to *taskquadruplet* and *Tasksingles*, respectively, the response time significantly decreases. The effect is, however, strongest from changing from *pairs* to *singles* (the respodents generally complete the singles tasks faster, relative to the quadruplet tasks).

  
3) Now add _pas_ and its interaction with _task_ to the fixed effects  

```{r}

# fails to converge: pastask <- lmer(log(rt.obj)~pas*task + (1 + task | subject) + (1 | trial), data = df_experiment, REML=FALSE) 

pastask <- lmer(log(rt.obj)~pas*task + (1 | subject) + (1 | trial), data = df_experiment, REML=FALSE) 

```

  i. how many types of group intercepts (random effects) can you add without ending up with convergence issues or singular fits?  
  
```{r}

pastask1 <- lmer(log(rt.obj) ~ task*pas + (1|subject), data = df_experiment, REML = FALSE)
pastask2 <- lmer(log(rt.obj) ~ task*pas + (1|trial) + (1|subject), data = df_experiment, REML = FALSE)

pastask3 <- lmer(log(rt.obj) ~ task*pas + (1|trial) + (1|subject) +  (1|target.contrast), data = df_experiment, REML = FALSE)
pastask4 <- lmer(log(rt.obj) ~ task*pas + (1|trial) + (1|subject) +  (1|cue), data = df_experiment, REML = FALSE)
pastask5 <- lmer(log(rt.obj) ~ task*pas + (1|trial) + (1|subject) +  (1|cue) + (1|target.contrast), data = df_experiment, REML = F)
```
  
The first 3 models (up to 3 group intercept) can be modelled without running into convergence issues if I add trial, subject and cue. However, if I try to add target.contrast as my third group intercept, I run into convergence issues. However, if adding cue as my third group intercept and target.contrast at my fourth, I can add 3 types (but run into issues at the fourth one).

  ii. create a model by adding random intercepts (without modelling slopes) that results in a singular fit - then use `print(VarCorr(<your.model>), comp='Variance')` to inspect the variance vector - explain why the fit is singular (Hint: read the first paragraph under details in the help for `isSingular`)
  
```{r}
print(VarCorr(pastask5), comp='Variance')

help(isSingular)
```
  
  iii. in your own words - how could you explain why your model would result in a singular fit? 

We can see that the variances of all the random intercepts in the model pastask5 are all very low (especially our random-effect variance estimate for "cue" is nearly zero). Variances being estimated as 0 results in singular fit. It indicates that the model is overfitting, so the structure of our random effects is too complex for our data to support it.
  

## Exercise 3

1) Initialise a new data frame, `data.count`. _count_ should indicate the number of times they categorized their experience as _pas_ 1-4 for each _task_. I.e. the data frame would have for subject 1: for task:singles, pas1 was used # times, pas2 was used # times, pas3 was used # times and pas4 was used # times. You would then do the same for task:pairs and task:quadruplet  

```{r}
pacman::p_load(dplyr)

data_count <- df %>% 
  group_by(subject, task, pas) %>% 
  dplyr::summarize("count" = n())

data_count
# the cause of this error is R’s confusion which summarize function (dplyr vs. plyr) it should use.
```


2) Now fit a multilevel model that models a unique "slope" for _pas_ for each _subject_ with the interaction between _pas_ and _task_ and their main effects being modelled  

```{r}

countmodel1 <- glmer(count~pas*task + (1+pas|subject), data = data_count, family = poisson, control = glmerControl(optimizer="bobyqa")) 

```
 

summary(model1)
  i. which family should be used?  
  
  Poisson Regression should be used as it is best for modeling a dependent variable that consists of "count data" given one or more independent variable. Poisson distributed data is integer-valued, discrete, not continuous, and is limited to non-negative values.  It also assumes that the errors follow a Poisson distribution rather than a normal distribution, and models the natural log of the response variable, ln(Y), as a linear function of the coefficients (rather than modeling Y as a linear function of the regression coefficients).
  
  ii. why is a slope for _pas_ not really being modelled?  
  
  _pas_ is encoded as a factor, meaning that it computes the analysis for each level seperately . Thus, we'll get a slope for each separate level of _pas_, where each slope is relative to the reference level. 
  
  
  iii. if you get a convergence error, try another algorithm (the default is the _Nelder_Mead_) - try (_bobyqa_) for which the `dfoptim` package is needed. In `glmer`, you can add the following for the `control` argument: `glmerControl(optimizer="bobyqa")` (if you are interested, also have a look at the function `allFit`)
  
  *The control argument "glmerControl(optimizer="bobyqa")" was added to avoid convergence error*
  
  iv. when you have a converging fit - fit a model with only the main effects of _pas_ and _task_. Compare this with the model that also includes the interaction  
  
```{r}
# Model with only the main effects of pas and task
countmodel2 <- glmer(count ~ pas + task + (1 + pas|subject), data = data_count, family = "poisson", control = glmerControl(optimizer="bobyqa")) 

rownames <- c("With interaction", "Without interaction")
AIC <- c(AIC(countmodel1), AIC(countmodel2))
resvar <- c(sum(residuals(countmodel1)^2),sum(residuals(countmodel2)^2)) #residual variance
res_sd<- c(sqrt((sum(residuals(countmodel1))^2/length(residuals(countmodel1))-2)),sqrt((sum(residuals(countmodel2))^2/length(residuals(countmodel2))-2))) # residual standard deviation 

as_tibble(cbind(rownames, AIC, resvar, res_sd))

```
  
The model that includes the interaction has the lowest AIC value as well as the lowest residual variance and residual standard deviation (as compared to the model fitted with only the main effects of pas and task). 

  v. indicate which of the two models, you would choose and why  
  
I would choose the model that includes the interaction. Partly because of the lower AIC value, residual variance and residual standard deviation, but also because it is easier to theoretically justify choosing this model. We are not really interested in predicting count from PAS rating (i.e. the participants confidence rating). Rather, we are interested in the interaction between task and pas rating, as PAS is related to the specific task at hand and we would expect PAS rating to depend on the task, considering the fact that the *_task_* variable indicates the number of potential targets that were cued (consisted of either 2,4, or 8 digit) and thus also indicates the difficulty of the task (and a more difficult task would be expected to generate a lower confidence rating).  


 vi. based on your chosen model - write a short report on what this says about the distribution of ratings as dependent on _pas_ and _task_  
 
```{r}
summary(countmodel1)
```
 
  
A general linear mixed effect model was fitted with a poisson link function to investigate the distribution of ratings (*count*) as dependent on *pas* and *task*. In other words, the model predicted our outcome variable *count* based on the fixed effects of *pas* and *task* as well as their interaction term. For random effects, *pas* was modelled as a random slope for each *subject*, with *subject* modeled as random intercept. The direction of the slope (whether it is positive or negative) is highly dependent on the interaction between pas and task.



vii. include a plot that shows the estimated amount of ratings for four subjects of your choosing 

```{r}
# Using the same 4 respondents as previously (3, 19, 12, 25)

data.count_four <- data_count %>% 
  filter(subject == "003" | subject == "019" | subject == "012" | subject == "025")
m_four <- glmer(count ~ pas * task + (pas|subject), data = data.count_four, family = poisson)
data.count_four %>% 
    ggplot() +
      geom_point(aes(x = pas, y = fitted(m_four), color = "Estimated")) + 
      geom_point(aes(x = pas, y = count, color = "Observed")) +
      facet_wrap( ~ subject)
  
```

  
3) Finally, fit a multilevel model that models _correct_ as dependent on _task_ with a unique intercept for each _subject_  

```{r}
finalmodel<- glmer(correct ~ task + (1|subject), family = "binomial", data = df_experiment)
summary(correct_model1)
```

   i. does _task_ explain performance?  
   
  
   Yes, task significantly explains performance (as measured by correctness) for all 3 task levels (p < .05 in all cases). In the quadruplets task, the performance is significantly worse as compared to the task pairs, but subjects' performance in the singles task is better as compared to pairs task. This would be expected as an increase in task complexity would be expected to result in a decrease in the amount of correct answers. 


  ii. add _pas_ as a main effect on top of _task_ - what are the consequences of that?  
  
```{r}
finalmodel2 <- glmer(correct ~ task + pas + (1 | subject), data = df_experiment, family = 'binomial')
```

Task looses is significance in explaining performance. Pas seems to be a better predictor (it significantly predicts *correct*). This makes sense as perceptual awareness theoretically must be believed to pressupose giving the correct answer (above the threshold that would be expected by chance alone.)
  
  
  iii. now fit a multilevel model that models _correct_ as dependent on _pas_ with a unique intercept for each _subject_
  
```{r}
finalmodel3 <- glmer(correct ~ pas + (1 | subject), data = df_experiment, family = 'binomial')
```
  
  iv. finally, fit a model that models the interaction between _task_ and _pas_  and their main effects  

```{r}

finalmodel4 <- glmer(correct ~ task*pas + (1 | subject), data = df_experiment, family = 'binomial')

```

  
  v. describe in your words which model is the best in explaining the variance in accuracy  
  
```{r}
rownames <- c("M1", "M2", "M3", "M4")
AIC <- c(AIC(finalmodel), AIC(finalmodel2), AIC(finalmodel3),AIC(finalmodel4))
resvar <- c(sum(residuals(finalmodel)^2),sum(residuals(finalmodel2)^2),sum(residuals(finalmodel3)^2),sum(residuals(finalmodel4)^2)) #residual variance
res_sd<- c(sqrt((sum(residuals(finalmodel))^2/length(residuals(finalmodel))-2)),sqrt((sum(residuals(finalmodel2))^2/length(residuals(finalmodel2))-2)),sqrt((sum(residuals(finalmodel3))^2/length(residuals(finalmodel3))-2)),sqrt((sum(residuals(finalmodel3))^2/length(residuals(finalmodel3))-2)),sqrt((sum(residuals(finalmodel4))^2/length(residuals(finalmodel4))-2))) # residual standard deviation 

as_tibble(cbind(rownames, AIC, resvar, res_sd))
anova(finalmodel, finalmodel2, finalmodel3,finalmodel4)

```

