---
title: "practical_exercise_1, Methods 3, 2021, autumn semester"
author: '[FILL IN YOUR NAME]'
date: "[FILL IN THE DATE]"
output: html_document
---

# 3) The General Linear Model

## Exercise 1
The general linear model: $Y = X \beta + \epsilon$:  
Do a linear regression, expressing fuel usage as a function of weight using the function __lm__  

```{r, eval=FALSE}

data(mtcars)
model <- lm(mpg~wt, data=mtcars)
summary(model)

```

1. extract $\hat{\beta}$, $Y$, $\hat{Y}$, $X$ and $\epsilon$ from __model__ (hint: have a look at the function __model.matrix__)  
    i. create a plot that illustrates $Y$ and $\hat{Y}$ (if you are feeling ambitious, also include $\epsilon$ (hint: you can use the function __arrows__))
    
First,we construct the design matrix. In this matrix, we have 1 in the first column (intercept). We have 32 observations, one for each predictor 1 in the design matrix' left column.

t(X) means we transpose our design matric
t(X) %*% X is doing matrix multiplacation
solve() is matrix inversion
```{r}

X <- model.matrix(model) # extracting X (our design matrix)
Y <- mtcars$mpg # extracting Y (from our dataset)
Y
bhat <- solve(t(X) %*% X) %*% t(X) %*% Y # our estimate of beta
bhat
yhat <- X %*% bhat
yhat
errors <- Y-yhat
```

We have now extracted 

$\hat{\beta}$ = behat
$Y$ = Y
$\hat{Y}$ = yhat 
$X$ = X
$\epsilon$ = errors


1.i: creating a plot that illustrates $Y$ and $\hat{Y}$
```{r}
# One way to plot it 
mtcars %>% 
  ggplot(aes(wt, mpg))+
  geom_point(aes(y = yhat), colour = "orange") +
  geom_point(aes(y = Y), colour = "darkred")


# Another way to plot it 
plot(Y, yhat)
abline(a = 0, b = 1)
arrows(Y, yhat, x1 = Y, y1 = Y, length = 0.1, angle = 3, code = 2, col = "blue", lty = par("lty"), lwd = par("lwd"))
title("Actual Y-values and Estimated Y-values")

```
In the first plot, the dark red dots signifies the actual Y values, whereas the orange dots signify the predicted values. 

In the second plot, the X axis represents the Actual values, whereas the Y axis represents the predicted values of Y. If our model predicted the y values perfectly, all dots would be placed on top of the abline. 

    
2. estimate $\beta$ for a quadratic model ($y = {\beta}_{2} x^2 + {\beta}_{1} x + {\beta}_{0}$) using ordinary least squares _without_ using __lm__; $\hat{\beta} = {({X}^{T} X)}^{-1} {X}^{T} Y$ (hint: add a third column to $X$ from step 1)

```{r}

n <- 32 
X <- cbind(rep(1, n), mtcars$wt, mtcars$wt^2) # creating a design matrix which has 32 rows and a column with 1's in row. I also add a column of our Y value, and one with our Y value squared. 


# I estimate the quadratic beta using the OLS estimator
beta_qua <- solve(t(X)%*%X)%*%t(X)%*%Y
beta_qua # intercept to slopes (en slope normal og en for vores squared værdi)

```

3. compare your acquired $\hat{\beta}$ with the output of the corresponding quadratic model created using __lm__ (hint: use the function __I__, see details under help and the sub-section formula operators here: https://www.datacamp.com/community/tutorials/r-formula-tutorial)  
    i. create a plot that illustrates $Y$ and $\hat{Y}$ (if you are feeling ambitious, also include $\epsilon$ (hint: you can use the function __arrows__))  
    
```{r}
compare <- lm(Y ~ X[,2] + X[,3], data=mtcars)

# comparison
compare
beta_qua

```
    
Comparing the above with beta_qua, I see that both our "compare" model and beta_qua gives the following coefficients: 49.93, -13.38, and 1.171.

```{r}

# Plotting actual vs estimated (quadratic) y values
yhatols <- X%*%beta_qua # finding y_hat
plot(Y, yhatols)
abline(a = 0, b = 1, col = 'darkgreen')

```
    

## Exercise 2

Compare the plotted quadratic fit to the linear fit  

1. which seems better?  

```{r}
summary(comparing)
```

2. calculate the sum of squared errors, (show the calculation based on $\epsilon$). Which fit has the lower sum?  

```{r}

```

3. now make a cubic fit ($y = {\beta}_{3} x^3 + {\beta}_{2} x^2 + {\beta}_{1} x + {\beta}_{0}$) and compare it to the quadratic fit  
    i. create a plot that illustrates $Y$ and $\hat{Y}$ for both the cubic and the quadratic fits (plot them in the same plot)  
    ii. compare the sum of squared errors  
    iii. what's the estimated value of the "cubic" (${\beta}_3$) parameter? Comment on this!  
    
```{r}

```
    
4. bonus question: which summary statistic is the fitted value (_Intercept_ or ${\beta}_0$ in $y = {\beta}_0$) below identical to?

```{r, echo=FALSE}
lm(mpg ~ 1, data=mtcars)
```

## Exercise 3
Doing a logistic regression - estimating the probability that a car has automatic transmission (0) or manual transmission (1) based on its weight
```{r, eval=FALSE}
data(mtcars)
logistic.model <- glm(formula=..., data=..., family='binomial)
```

Probabilities live on the range $(0, 1)$ - using the so-called logit function as a "link-function" we can map these onto the range $(-\infty, \infty)$, i.e. the real numbers.  
  
What we model in this case is: $Pr(y = 1) = logit^{-1}(X \beta)$, i.e. the probability of a car having manual transmission, given its weight. $X \beta$ is called the linear predictor; compare with $Y = X \beta + \epsilon$ 
It is helpful to define the logit function and its inverse function for the following:  

```{r}
logit <-     function(x) log(x / (1 - x))
inv.logit <- function(x) exp(x) / (1 + exp(x))
```

1. plot the fitted values for __logistic.model__:  
    i. what is the relation between the __linear.predictors__ and the __fitted_values__ of the __logistic.model__ object?
2. plot the logistic function, you've estimated based on your $\hat{\beta}$, (not just the fitted values). Use an _xlim_ of (0, 7)
    i. what's the interpretation of the estimated $\hat{\beta}_0$ (the _Intercept_)
    ii. calculate the estimated probability that the Pontiac Firebird has automatic transmission, given its weight
    iii. bonus question - plot the logistic function and highlight all the cars where we guessed wrongly, if we used the following "quantizer" function:
    
\begin{equation}
  transmission_{guess}=
  \begin{cases}
    1 (manual), & \text{if}\ PR(y = 1) ≥ 0.5 \\
    0 (automatic), & \text{otherwise}
  \end{cases}
\end{equation}    
    
3. plot quadratic fit alongside linear fit  
    i. judging visually, does adding a quadratic term make a difference?
    ii. check the details in the help of the AIC function - which of the models provide the better fit according to the AIC values and the residual deviance respectively?
    iii. in your own words, why might it be good to penalise a model like the quadratic model, we just fitted.
    
# Next time
We are going to looking at extending our models with so called random effects. We need to install the package "lme4" for this. Run the code below or install it from your package manager (Linux)  
```{r, eval=FALSE}
install.packages("lme4")
```
We can fit a model like this:

```{r}
library(lme4)
mixed.model <- lmer(mpg ~ wt + (1 | cyl), data=mtcars)
```

They result in plots like these:
```{r}
par(font.lab=2, font.axis=2, cex=1.2)
plot(mtcars$wt, fitted.values(mixed.model),
     main='Linear regression with group intercepts (n cylinders)',
    xlab='Weight (lb/1000)', ylab='Miles/(US) gallon',
    pch=3)
```

and this
```{r}
mixed.model <- lmer(mpg ~ wt + (wt | cyl), data=mtcars)
plot(mtcars$wt, fitted.values(mixed.model),
     main='Linear regression with group intercepts and group slopes (n cylinders)',
    xlab='Weight (lb/1000)', ylab='Miles/(US) gallon',
    pch=3)
``` 

but also new warnings like:  

Warning:
In checkConv(attr(opt, "derivs"), opt\$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.0121962 (tol = 0.002, component 1)
